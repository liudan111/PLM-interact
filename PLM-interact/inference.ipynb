{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel,AutoModelForMaskedLM,AutoTokenizer,EsmConfig\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PLMinteract(nn.Module):\n",
    "  def __init__(self,model_name,num_labels,embedding_size): \n",
    "    super(PLMinteract,self).__init__() \n",
    "    self.esm_mask = AutoModelForMaskedLM.from_pretrained(model_name) \n",
    "    self.embedding_size=embedding_size\n",
    "    self.classifier = nn.Linear(embedding_size,1) # embedding_size \n",
    "    self.num_labels=num_labels\n",
    "\n",
    "  def forward_test(self,features):\n",
    "    embedding_output = self.esm_mask.base_model(**features, return_dict=True)\n",
    "    embedding=embedding_output.last_hidden_state[:,0,:] #cls token\n",
    "    embedding = F.relu(embedding)\n",
    "    logits = self.classifier(embedding)\n",
    "    logits=logits.view(-1)\n",
    "    probability = torch.sigmoid(logits)\n",
    "    return  probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein1 =\"EGCVSNLMVCNLAYSGKLEELKESILADKSLATRTDQDSRTALHWACSAGHTEIVEFLLQLGVPVNDKDDAGWSPLHIAASAGRDEIVKALLGKGAQVNAVNQNGCTPLHYAASKNRHEIAVMLLEGGANPDAKDHYEATAMHRAAAKGNLKMIHILLYYKASTNIQDTEGNTPLHLACDEERVEEAKLLVSQGASIYIENKEEKTPLQVAKGGLGLILKRMVEG\"\n",
    "protein2= \"MGQSQSGGHGPGGGKKDDKDKKKKYEPPVPTRVGKKKKKTKGPDAASKLPLVTPHTQCRLKLLKLERIKDYLLMEEEFIRNQEQMKPLEEKQEEERSKVDDLRGTPMSVGTLEEIIDDNHAIVSTSVGSEHYVSILSFVDKDLLEPGCSVLLNHKVHAVIGVLMDDTDPLVTVMKVEKAPQETYADIGGLDNQIQEIKESVELPLTHPEYYEEMGIKPPKGVILYGPPGTGKTLLAKAVANQTSATFLRVVGSELIQKYLGDGPKLVRELFRVAEEHAPSIVFIDEIDAIGTKRYDSNSGGEREIQRTMLELLNQLDGFDSRGDVKVIMATNRIETLDPALIRPGRIDRKIEFPLPDEKTKKRIFQIHTSRMTLADDVTLDDLIMAKDDLSGADIKAICTEAGLMALRERRMKVTNEDFKKSKENVLYKKQEGTPEGLYL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /mnt/data/project0019/danliu/PPI/offline/esm2_t12_35M_UR50D were not used when initializing EsmModel: ['contact_head.regression.weight', 'contact_head.regression.bias']\n",
      "- This IS expected if you are initializing EsmModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# model_name= '/mnt/data/project0019/danliu/PPI/offline/esm2_t33_650M_UR50D'\n",
    "# folder_huggingface_download='/mnt/data/project0019/danliu/PPI/checkpoint/650m/dscript/task6/2024-03-22_12-00-48-1mask_10classi_dscript_1plus/huggingface_model'\n",
    "\n",
    "model_name= '/mnt/data/project0019/danliu/PPI/offline/esm2_t12_35M_UR50D'\n",
    "folder_huggingface_download='/mnt/data/project0019/danliu/PPI/checkpoint/35m/dscript/task2/huggingface_model'\n",
    "\n",
    "config = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_size =1280\n",
    "# PLMinter= PLMinteract(model_name, 1, config, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.esm.configuration_esm.EsmConfig'> for this kind of AutoModel: AutoModelForMaskedLM.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, CamembertConfig, ConvBertConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, FlaubertConfig, FNetConfig, FunnelConfig, IBertConfig, LayoutLMConfig, LongformerConfig, LukeConfig, MBartConfig, MegatronBertConfig, MobileBertConfig, MPNetConfig, MvpConfig, NezhaConfig, NystromformerConfig, PerceiverConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RoFormerConfig, SqueezeBertConfig, TapasConfig, Wav2Vec2Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, YosoConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m embedding_size \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m480\u001b[39m\n\u001b[0;32m----> 2\u001b[0m PLMinter\u001b[38;5;241m=\u001b[39m \u001b[43mPLMinteract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[60], line 10\u001b[0m, in \u001b[0;36mPLMinteract.__init__\u001b[0;34m(self, model_name, num_labels, embedding_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,model_name,num_labels,embedding_size): \n\u001b[1;32m      9\u001b[0m   \u001b[38;5;28msuper\u001b[39m(PLMinteract,\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m() \n\u001b[0;32m---> 10\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mesm_mask \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForMaskedLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_size\u001b[38;5;241m=\u001b[39membedding_size\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embedding_size,\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# embedding_size \u001b[39;00m\n",
      "File \u001b[0;32m/opt/gridware/depots/996bcebb/el7/pkg/apps/anaconda3/2023.03/bin/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:466\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    464\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    465\u001b[0m     )\n\u001b[0;32m--> 466\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.esm.configuration_esm.EsmConfig'> for this kind of AutoModel: AutoModelForMaskedLM.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, CamembertConfig, ConvBertConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, FlaubertConfig, FNetConfig, FunnelConfig, IBertConfig, LayoutLMConfig, LongformerConfig, LukeConfig, MBartConfig, MegatronBertConfig, MobileBertConfig, MPNetConfig, MvpConfig, NezhaConfig, NystromformerConfig, PerceiverConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RoFormerConfig, SqueezeBertConfig, TapasConfig, Wav2Vec2Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, YosoConfig."
     ]
    }
   ],
   "source": [
    "embedding_size =480\n",
    "PLMinter= PLMinteract(model_name, 1, embedding_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PLMinteract(\n",
       "  (esm_mask): EsmModel(\n",
       "    (embeddings): EsmEmbeddings(\n",
       "      (word_embeddings): Embedding(33, 480, padding_idx=1)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (position_embeddings): Embedding(1026, 480, padding_idx=1)\n",
       "    )\n",
       "    (encoder): EsmEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (emb_layer_norm_after): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (pooler): EsmPooler(\n",
       "      (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=480, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PLMinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PLMinteract:\n\tMissing key(s) in state_dict: \"esm_mask.embeddings.position_ids\", \"esm_mask.embeddings.word_embeddings.weight\", \"esm_mask.embeddings.position_embeddings.weight\", \"esm_mask.encoder.layer.0.attention.self.query.weight\", \"esm_mask.encoder.layer.0.attention.self.query.bias\", \"esm_mask.encoder.layer.0.attention.self.key.weight\", \"esm_mask.encoder.layer.0.attention.self.key.bias\", \"esm_mask.encoder.layer.0.attention.self.value.weight\", \"esm_mask.encoder.layer.0.attention.self.value.bias\", \"esm_mask.encoder.layer.0.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.0.attention.output.dense.weight\", \"esm_mask.encoder.layer.0.attention.output.dense.bias\", \"esm_mask.encoder.layer.0.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.0.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.0.intermediate.dense.weight\", \"esm_mask.encoder.layer.0.intermediate.dense.bias\", \"esm_mask.encoder.layer.0.output.dense.weight\", \"esm_mask.encoder.layer.0.output.dense.bias\", \"esm_mask.encoder.layer.0.LayerNorm.weight\", \"esm_mask.encoder.layer.0.LayerNorm.bias\", \"esm_mask.encoder.layer.1.attention.self.query.weight\", \"esm_mask.encoder.layer.1.attention.self.query.bias\", \"esm_mask.encoder.layer.1.attention.self.key.weight\", \"esm_mask.encoder.layer.1.attention.self.key.bias\", \"esm_mask.encoder.layer.1.attention.self.value.weight\", \"esm_mask.encoder.layer.1.attention.self.value.bias\", \"esm_mask.encoder.layer.1.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.1.attention.output.dense.weight\", \"esm_mask.encoder.layer.1.attention.output.dense.bias\", \"esm_mask.encoder.layer.1.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.1.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.1.intermediate.dense.weight\", \"esm_mask.encoder.layer.1.intermediate.dense.bias\", \"esm_mask.encoder.layer.1.output.dense.weight\", \"esm_mask.encoder.layer.1.output.dense.bias\", \"esm_mask.encoder.layer.1.LayerNorm.weight\", \"esm_mask.encoder.layer.1.LayerNorm.bias\", \"esm_mask.encoder.layer.2.attention.self.query.weight\", \"esm_mask.encoder.layer.2.attention.self.query.bias\", \"esm_mask.encoder.layer.2.attention.self.key.weight\", \"esm_mask.encoder.layer.2.attention.self.key.bias\", \"esm_mask.encoder.layer.2.attention.self.value.weight\", \"esm_mask.encoder.layer.2.attention.self.value.bias\", \"esm_mask.encoder.layer.2.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.2.attention.output.dense.weight\", \"esm_mask.encoder.layer.2.attention.output.dense.bias\", \"esm_mask.encoder.layer.2.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.2.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.2.intermediate.dense.weight\", \"esm_mask.encoder.layer.2.intermediate.dense.bias\", \"esm_mask.encoder.layer.2.output.dense.weight\", \"esm_mask.encoder.layer.2.output.dense.bias\", \"esm_mask.encoder.layer.2.LayerNorm.weight\", \"esm_mask.encoder.layer.2.LayerNorm.bias\", \"esm_mask.encoder.layer.3.attention.self.query.weight\", \"esm_mask.encoder.layer.3.attention.self.query.bias\", \"esm_mask.encoder.layer.3.attention.self.key.weight\", \"esm_mask.encoder.layer.3.attention.self.key.bias\", \"esm_mask.encoder.layer.3.attention.self.value.weight\", \"esm_mask.encoder.layer.3.attention.self.value.bias\", \"esm_mask.encoder.layer.3.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.3.attention.output.dense.weight\", \"esm_mask.encoder.layer.3.attention.output.dense.bias\", \"esm_mask.encoder.layer.3.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.3.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.3.intermediate.dense.weight\", \"esm_mask.encoder.layer.3.intermediate.dense.bias\", \"esm_mask.encoder.layer.3.output.dense.weight\", \"esm_mask.encoder.layer.3.output.dense.bias\", \"esm_mask.encoder.layer.3.LayerNorm.weight\", \"esm_mask.encoder.layer.3.LayerNorm.bias\", \"esm_mask.encoder.layer.4.attention.self.query.weight\", \"esm_mask.encoder.layer.4.attention.self.query.bias\", \"esm_mask.encoder.layer.4.attention.self.key.weight\", \"esm_mask.encoder.layer.4.attention.self.key.bias\", \"esm_mask.encoder.layer.4.attention.self.value.weight\", \"esm_mask.encoder.layer.4.attention.self.value.bias\", \"esm_mask.encoder.layer.4.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.4.attention.output.dense.weight\", \"esm_mask.encoder.layer.4.attention.output.dense.bias\", \"esm_mask.encoder.layer.4.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.4.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.4.intermediate.dense.weight\", \"esm_mask.encoder.layer.4.intermediate.dense.bias\", \"esm_mask.encoder.layer.4.output.dense.weight\", \"esm_mask.encoder.layer.4.output.dense.bias\", \"esm_mask.encoder.layer.4.LayerNorm.weight\", \"esm_mask.encoder.layer.4.LayerNorm.bias\", \"esm_mask.encoder.layer.5.attention.self.query.weight\", \"esm_mask.encoder.layer.5.attention.self.query.bias\", \"esm_mask.encoder.layer.5.attention.self.key.weight\", \"esm_mask.encoder.layer.5.attention.self.key.bias\", \"esm_mask.encoder.layer.5.attention.self.value.weight\", \"esm_mask.encoder.layer.5.attention.self.value.bias\", \"esm_mask.encoder.layer.5.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.5.attention.output.dense.weight\", \"esm_mask.encoder.layer.5.attention.output.dense.bias\", \"esm_mask.encoder.layer.5.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.5.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.5.intermediate.dense.weight\", \"esm_mask.encoder.layer.5.intermediate.dense.bias\", \"esm_mask.encoder.layer.5.output.dense.weight\", \"esm_mask.encoder.layer.5.output.dense.bias\", \"esm_mask.encoder.layer.5.LayerNorm.weight\", \"esm_mask.encoder.layer.5.LayerNorm.bias\", \"esm_mask.encoder.layer.6.attention.self.query.weight\", \"esm_mask.encoder.layer.6.attention.self.query.bias\", \"esm_mask.encoder.layer.6.attention.self.key.weight\", \"esm_mask.encoder.layer.6.attention.self.key.bias\", \"esm_mask.encoder.layer.6.attention.self.value.weight\", \"esm_mask.encoder.layer.6.attention.self.value.bias\", \"esm_mask.encoder.layer.6.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.6.attention.output.dense.weight\", \"esm_mask.encoder.layer.6.attention.output.dense.bias\", \"esm_mask.encoder.layer.6.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.6.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.6.intermediate.dense.weight\", \"esm_mask.encoder.layer.6.intermediate.dense.bias\", \"esm_mask.encoder.layer.6.output.dense.weight\", \"esm_mask.encoder.layer.6.output.dense.bias\", \"esm_mask.encoder.layer.6.LayerNorm.weight\", \"esm_mask.encoder.layer.6.LayerNorm.bias\", \"esm_mask.encoder.layer.7.attention.self.query.weight\", \"esm_mask.encoder.layer.7.attention.self.query.bias\", \"esm_mask.encoder.layer.7.attention.self.key.weight\", \"esm_mask.encoder.layer.7.attention.self.key.bias\", \"esm_mask.encoder.layer.7.attention.self.value.weight\", \"esm_mask.encoder.layer.7.attention.self.value.bias\", \"esm_mask.encoder.layer.7.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.7.attention.output.dense.weight\", \"esm_mask.encoder.layer.7.attention.output.dense.bias\", \"esm_mask.encoder.layer.7.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.7.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.7.intermediate.dense.weight\", \"esm_mask.encoder.layer.7.intermediate.dense.bias\", \"esm_mask.encoder.layer.7.output.dense.weight\", \"esm_mask.encoder.layer.7.output.dense.bias\", \"esm_mask.encoder.layer.7.LayerNorm.weight\", \"esm_mask.encoder.layer.7.LayerNorm.bias\", \"esm_mask.encoder.layer.8.attention.self.query.weight\", \"esm_mask.encoder.layer.8.attention.self.query.bias\", \"esm_mask.encoder.layer.8.attention.self.key.weight\", \"esm_mask.encoder.layer.8.attention.self.key.bias\", \"esm_mask.encoder.layer.8.attention.self.value.weight\", \"esm_mask.encoder.layer.8.attention.self.value.bias\", \"esm_mask.encoder.layer.8.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.8.attention.output.dense.weight\", \"esm_mask.encoder.layer.8.attention.output.dense.bias\", \"esm_mask.encoder.layer.8.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.8.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.8.intermediate.dense.weight\", \"esm_mask.encoder.layer.8.intermediate.dense.bias\", \"esm_mask.encoder.layer.8.output.dense.weight\", \"esm_mask.encoder.layer.8.output.dense.bias\", \"esm_mask.encoder.layer.8.LayerNorm.weight\", \"esm_mask.encoder.layer.8.LayerNorm.bias\", \"esm_mask.encoder.layer.9.attention.self.query.weight\", \"esm_mask.encoder.layer.9.attention.self.query.bias\", \"esm_mask.encoder.layer.9.attention.self.key.weight\", \"esm_mask.encoder.layer.9.attention.self.key.bias\", \"esm_mask.encoder.layer.9.attention.self.value.weight\", \"esm_mask.encoder.layer.9.attention.self.value.bias\", \"esm_mask.encoder.layer.9.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.9.attention.output.dense.weight\", \"esm_mask.encoder.layer.9.attention.output.dense.bias\", \"esm_mask.encoder.layer.9.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.9.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.9.intermediate.dense.weight\", \"esm_mask.encoder.layer.9.intermediate.dense.bias\", \"esm_mask.encoder.layer.9.output.dense.weight\", \"esm_mask.encoder.layer.9.output.dense.bias\", \"esm_mask.encoder.layer.9.LayerNorm.weight\", \"esm_mask.encoder.layer.9.LayerNorm.bias\", \"esm_mask.encoder.layer.10.attention.self.query.weight\", \"esm_mask.encoder.layer.10.attention.self.query.bias\", \"esm_mask.encoder.layer.10.attention.self.key.weight\", \"esm_mask.encoder.layer.10.attention.self.key.bias\", \"esm_mask.encoder.layer.10.attention.self.value.weight\", \"esm_mask.encoder.layer.10.attention.self.value.bias\", \"esm_mask.encoder.layer.10.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.10.attention.output.dense.weight\", \"esm_mask.encoder.layer.10.attention.output.dense.bias\", \"esm_mask.encoder.layer.10.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.10.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.10.intermediate.dense.weight\", \"esm_mask.encoder.layer.10.intermediate.dense.bias\", \"esm_mask.encoder.layer.10.output.dense.weight\", \"esm_mask.encoder.layer.10.output.dense.bias\", \"esm_mask.encoder.layer.10.LayerNorm.weight\", \"esm_mask.encoder.layer.10.LayerNorm.bias\", \"esm_mask.encoder.layer.11.attention.self.query.weight\", \"esm_mask.encoder.layer.11.attention.self.query.bias\", \"esm_mask.encoder.layer.11.attention.self.key.weight\", \"esm_mask.encoder.layer.11.attention.self.key.bias\", \"esm_mask.encoder.layer.11.attention.self.value.weight\", \"esm_mask.encoder.layer.11.attention.self.value.bias\", \"esm_mask.encoder.layer.11.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.11.attention.output.dense.weight\", \"esm_mask.encoder.layer.11.attention.output.dense.bias\", \"esm_mask.encoder.layer.11.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.11.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.11.intermediate.dense.weight\", \"esm_mask.encoder.layer.11.intermediate.dense.bias\", \"esm_mask.encoder.layer.11.output.dense.weight\", \"esm_mask.encoder.layer.11.output.dense.bias\", \"esm_mask.encoder.layer.11.LayerNorm.weight\", \"esm_mask.encoder.layer.11.LayerNorm.bias\", \"esm_mask.encoder.emb_layer_norm_after.weight\", \"esm_mask.encoder.emb_layer_norm_after.bias\", \"esm_mask.pooler.dense.weight\", \"esm_mask.pooler.dense.bias\". \n\tUnexpected key(s) in state_dict: \"esm_mask.esm.embeddings.word_embeddings.weight\", \"esm_mask.esm.embeddings.position_embeddings.weight\", \"esm_mask.esm.encoder.layer.0.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.0.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.0.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.0.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.0.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.0.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.0.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.0.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.0.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.0.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.0.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.0.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.0.output.dense.weight\", \"esm_mask.esm.encoder.layer.0.output.dense.bias\", \"esm_mask.esm.encoder.layer.0.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.0.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.1.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.1.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.1.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.1.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.1.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.1.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.1.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.1.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.1.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.1.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.1.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.1.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.1.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.1.output.dense.weight\", \"esm_mask.esm.encoder.layer.1.output.dense.bias\", \"esm_mask.esm.encoder.layer.1.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.1.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.2.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.2.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.2.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.2.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.2.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.2.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.2.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.2.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.2.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.2.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.2.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.2.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.2.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.2.output.dense.weight\", \"esm_mask.esm.encoder.layer.2.output.dense.bias\", \"esm_mask.esm.encoder.layer.2.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.2.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.3.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.3.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.3.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.3.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.3.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.3.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.3.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.3.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.3.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.3.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.3.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.3.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.3.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.3.output.dense.weight\", \"esm_mask.esm.encoder.layer.3.output.dense.bias\", \"esm_mask.esm.encoder.layer.3.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.3.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.4.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.4.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.4.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.4.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.4.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.4.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.4.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.4.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.4.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.4.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.4.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.4.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.4.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.4.output.dense.weight\", \"esm_mask.esm.encoder.layer.4.output.dense.bias\", \"esm_mask.esm.encoder.layer.4.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.4.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.5.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.5.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.5.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.5.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.5.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.5.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.5.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.5.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.5.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.5.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.5.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.5.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.5.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.5.output.dense.weight\", \"esm_mask.esm.encoder.layer.5.output.dense.bias\", \"esm_mask.esm.encoder.layer.5.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.5.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.6.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.6.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.6.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.6.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.6.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.6.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.6.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.6.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.6.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.6.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.6.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.6.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.6.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.6.output.dense.weight\", \"esm_mask.esm.encoder.layer.6.output.dense.bias\", \"esm_mask.esm.encoder.layer.6.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.6.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.7.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.7.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.7.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.7.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.7.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.7.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.7.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.7.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.7.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.7.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.7.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.7.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.7.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.7.output.dense.weight\", \"esm_mask.esm.encoder.layer.7.output.dense.bias\", \"esm_mask.esm.encoder.layer.7.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.7.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.8.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.8.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.8.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.8.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.8.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.8.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.8.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.8.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.8.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.8.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.8.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.8.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.8.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.8.output.dense.weight\", \"esm_mask.esm.encoder.layer.8.output.dense.bias\", \"esm_mask.esm.encoder.layer.8.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.8.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.9.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.9.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.9.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.9.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.9.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.9.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.9.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.9.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.9.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.9.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.9.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.9.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.9.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.9.output.dense.weight\", \"esm_mask.esm.encoder.layer.9.output.dense.bias\", \"esm_mask.esm.encoder.layer.9.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.9.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.10.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.10.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.10.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.10.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.10.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.10.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.10.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.10.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.10.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.10.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.10.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.10.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.10.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.10.output.dense.weight\", \"esm_mask.esm.encoder.layer.10.output.dense.bias\", \"esm_mask.esm.encoder.layer.10.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.10.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.11.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.11.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.11.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.11.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.11.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.11.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.11.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.11.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.11.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.11.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.11.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.11.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.11.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.11.output.dense.weight\", \"esm_mask.esm.encoder.layer.11.output.dense.bias\", \"esm_mask.esm.encoder.layer.11.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.11.LayerNorm.bias\", \"esm_mask.esm.encoder.emb_layer_norm_after.weight\", \"esm_mask.esm.encoder.emb_layer_norm_after.bias\", \"esm_mask.esm.contact_head.regression.weight\", \"esm_mask.esm.contact_head.regression.bias\", \"esm_mask.lm_head.bias\", \"esm_mask.lm_head.dense.weight\", \"esm_mask.lm_head.dense.bias\", \"esm_mask.lm_head.layer_norm.weight\", \"esm_mask.lm_head.layer_norm.bias\", \"esm_mask.lm_head.decoder.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m load_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_huggingface_download\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/pytorch_model.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mPLMinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/gridware/depots/996bcebb/el7/pkg/apps/anaconda3/2023.03/bin/lib/python3.10/site-packages/torch/nn/modules/module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1600\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1601\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1605\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1606\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PLMinteract:\n\tMissing key(s) in state_dict: \"esm_mask.embeddings.position_ids\", \"esm_mask.embeddings.word_embeddings.weight\", \"esm_mask.embeddings.position_embeddings.weight\", \"esm_mask.encoder.layer.0.attention.self.query.weight\", \"esm_mask.encoder.layer.0.attention.self.query.bias\", \"esm_mask.encoder.layer.0.attention.self.key.weight\", \"esm_mask.encoder.layer.0.attention.self.key.bias\", \"esm_mask.encoder.layer.0.attention.self.value.weight\", \"esm_mask.encoder.layer.0.attention.self.value.bias\", \"esm_mask.encoder.layer.0.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.0.attention.output.dense.weight\", \"esm_mask.encoder.layer.0.attention.output.dense.bias\", \"esm_mask.encoder.layer.0.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.0.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.0.intermediate.dense.weight\", \"esm_mask.encoder.layer.0.intermediate.dense.bias\", \"esm_mask.encoder.layer.0.output.dense.weight\", \"esm_mask.encoder.layer.0.output.dense.bias\", \"esm_mask.encoder.layer.0.LayerNorm.weight\", \"esm_mask.encoder.layer.0.LayerNorm.bias\", \"esm_mask.encoder.layer.1.attention.self.query.weight\", \"esm_mask.encoder.layer.1.attention.self.query.bias\", \"esm_mask.encoder.layer.1.attention.self.key.weight\", \"esm_mask.encoder.layer.1.attention.self.key.bias\", \"esm_mask.encoder.layer.1.attention.self.value.weight\", \"esm_mask.encoder.layer.1.attention.self.value.bias\", \"esm_mask.encoder.layer.1.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.1.attention.output.dense.weight\", \"esm_mask.encoder.layer.1.attention.output.dense.bias\", \"esm_mask.encoder.layer.1.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.1.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.1.intermediate.dense.weight\", \"esm_mask.encoder.layer.1.intermediate.dense.bias\", \"esm_mask.encoder.layer.1.output.dense.weight\", \"esm_mask.encoder.layer.1.output.dense.bias\", \"esm_mask.encoder.layer.1.LayerNorm.weight\", \"esm_mask.encoder.layer.1.LayerNorm.bias\", \"esm_mask.encoder.layer.2.attention.self.query.weight\", \"esm_mask.encoder.layer.2.attention.self.query.bias\", \"esm_mask.encoder.layer.2.attention.self.key.weight\", \"esm_mask.encoder.layer.2.attention.self.key.bias\", \"esm_mask.encoder.layer.2.attention.self.value.weight\", \"esm_mask.encoder.layer.2.attention.self.value.bias\", \"esm_mask.encoder.layer.2.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.2.attention.output.dense.weight\", \"esm_mask.encoder.layer.2.attention.output.dense.bias\", \"esm_mask.encoder.layer.2.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.2.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.2.intermediate.dense.weight\", \"esm_mask.encoder.layer.2.intermediate.dense.bias\", \"esm_mask.encoder.layer.2.output.dense.weight\", \"esm_mask.encoder.layer.2.output.dense.bias\", \"esm_mask.encoder.layer.2.LayerNorm.weight\", \"esm_mask.encoder.layer.2.LayerNorm.bias\", \"esm_mask.encoder.layer.3.attention.self.query.weight\", \"esm_mask.encoder.layer.3.attention.self.query.bias\", \"esm_mask.encoder.layer.3.attention.self.key.weight\", \"esm_mask.encoder.layer.3.attention.self.key.bias\", \"esm_mask.encoder.layer.3.attention.self.value.weight\", \"esm_mask.encoder.layer.3.attention.self.value.bias\", \"esm_mask.encoder.layer.3.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.3.attention.output.dense.weight\", \"esm_mask.encoder.layer.3.attention.output.dense.bias\", \"esm_mask.encoder.layer.3.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.3.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.3.intermediate.dense.weight\", \"esm_mask.encoder.layer.3.intermediate.dense.bias\", \"esm_mask.encoder.layer.3.output.dense.weight\", \"esm_mask.encoder.layer.3.output.dense.bias\", \"esm_mask.encoder.layer.3.LayerNorm.weight\", \"esm_mask.encoder.layer.3.LayerNorm.bias\", \"esm_mask.encoder.layer.4.attention.self.query.weight\", \"esm_mask.encoder.layer.4.attention.self.query.bias\", \"esm_mask.encoder.layer.4.attention.self.key.weight\", \"esm_mask.encoder.layer.4.attention.self.key.bias\", \"esm_mask.encoder.layer.4.attention.self.value.weight\", \"esm_mask.encoder.layer.4.attention.self.value.bias\", \"esm_mask.encoder.layer.4.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.4.attention.output.dense.weight\", \"esm_mask.encoder.layer.4.attention.output.dense.bias\", \"esm_mask.encoder.layer.4.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.4.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.4.intermediate.dense.weight\", \"esm_mask.encoder.layer.4.intermediate.dense.bias\", \"esm_mask.encoder.layer.4.output.dense.weight\", \"esm_mask.encoder.layer.4.output.dense.bias\", \"esm_mask.encoder.layer.4.LayerNorm.weight\", \"esm_mask.encoder.layer.4.LayerNorm.bias\", \"esm_mask.encoder.layer.5.attention.self.query.weight\", \"esm_mask.encoder.layer.5.attention.self.query.bias\", \"esm_mask.encoder.layer.5.attention.self.key.weight\", \"esm_mask.encoder.layer.5.attention.self.key.bias\", \"esm_mask.encoder.layer.5.attention.self.value.weight\", \"esm_mask.encoder.layer.5.attention.self.value.bias\", \"esm_mask.encoder.layer.5.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.5.attention.output.dense.weight\", \"esm_mask.encoder.layer.5.attention.output.dense.bias\", \"esm_mask.encoder.layer.5.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.5.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.5.intermediate.dense.weight\", \"esm_mask.encoder.layer.5.intermediate.dense.bias\", \"esm_mask.encoder.layer.5.output.dense.weight\", \"esm_mask.encoder.layer.5.output.dense.bias\", \"esm_mask.encoder.layer.5.LayerNorm.weight\", \"esm_mask.encoder.layer.5.LayerNorm.bias\", \"esm_mask.encoder.layer.6.attention.self.query.weight\", \"esm_mask.encoder.layer.6.attention.self.query.bias\", \"esm_mask.encoder.layer.6.attention.self.key.weight\", \"esm_mask.encoder.layer.6.attention.self.key.bias\", \"esm_mask.encoder.layer.6.attention.self.value.weight\", \"esm_mask.encoder.layer.6.attention.self.value.bias\", \"esm_mask.encoder.layer.6.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.6.attention.output.dense.weight\", \"esm_mask.encoder.layer.6.attention.output.dense.bias\", \"esm_mask.encoder.layer.6.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.6.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.6.intermediate.dense.weight\", \"esm_mask.encoder.layer.6.intermediate.dense.bias\", \"esm_mask.encoder.layer.6.output.dense.weight\", \"esm_mask.encoder.layer.6.output.dense.bias\", \"esm_mask.encoder.layer.6.LayerNorm.weight\", \"esm_mask.encoder.layer.6.LayerNorm.bias\", \"esm_mask.encoder.layer.7.attention.self.query.weight\", \"esm_mask.encoder.layer.7.attention.self.query.bias\", \"esm_mask.encoder.layer.7.attention.self.key.weight\", \"esm_mask.encoder.layer.7.attention.self.key.bias\", \"esm_mask.encoder.layer.7.attention.self.value.weight\", \"esm_mask.encoder.layer.7.attention.self.value.bias\", \"esm_mask.encoder.layer.7.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.7.attention.output.dense.weight\", \"esm_mask.encoder.layer.7.attention.output.dense.bias\", \"esm_mask.encoder.layer.7.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.7.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.7.intermediate.dense.weight\", \"esm_mask.encoder.layer.7.intermediate.dense.bias\", \"esm_mask.encoder.layer.7.output.dense.weight\", \"esm_mask.encoder.layer.7.output.dense.bias\", \"esm_mask.encoder.layer.7.LayerNorm.weight\", \"esm_mask.encoder.layer.7.LayerNorm.bias\", \"esm_mask.encoder.layer.8.attention.self.query.weight\", \"esm_mask.encoder.layer.8.attention.self.query.bias\", \"esm_mask.encoder.layer.8.attention.self.key.weight\", \"esm_mask.encoder.layer.8.attention.self.key.bias\", \"esm_mask.encoder.layer.8.attention.self.value.weight\", \"esm_mask.encoder.layer.8.attention.self.value.bias\", \"esm_mask.encoder.layer.8.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.8.attention.output.dense.weight\", \"esm_mask.encoder.layer.8.attention.output.dense.bias\", \"esm_mask.encoder.layer.8.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.8.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.8.intermediate.dense.weight\", \"esm_mask.encoder.layer.8.intermediate.dense.bias\", \"esm_mask.encoder.layer.8.output.dense.weight\", \"esm_mask.encoder.layer.8.output.dense.bias\", \"esm_mask.encoder.layer.8.LayerNorm.weight\", \"esm_mask.encoder.layer.8.LayerNorm.bias\", \"esm_mask.encoder.layer.9.attention.self.query.weight\", \"esm_mask.encoder.layer.9.attention.self.query.bias\", \"esm_mask.encoder.layer.9.attention.self.key.weight\", \"esm_mask.encoder.layer.9.attention.self.key.bias\", \"esm_mask.encoder.layer.9.attention.self.value.weight\", \"esm_mask.encoder.layer.9.attention.self.value.bias\", \"esm_mask.encoder.layer.9.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.9.attention.output.dense.weight\", \"esm_mask.encoder.layer.9.attention.output.dense.bias\", \"esm_mask.encoder.layer.9.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.9.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.9.intermediate.dense.weight\", \"esm_mask.encoder.layer.9.intermediate.dense.bias\", \"esm_mask.encoder.layer.9.output.dense.weight\", \"esm_mask.encoder.layer.9.output.dense.bias\", \"esm_mask.encoder.layer.9.LayerNorm.weight\", \"esm_mask.encoder.layer.9.LayerNorm.bias\", \"esm_mask.encoder.layer.10.attention.self.query.weight\", \"esm_mask.encoder.layer.10.attention.self.query.bias\", \"esm_mask.encoder.layer.10.attention.self.key.weight\", \"esm_mask.encoder.layer.10.attention.self.key.bias\", \"esm_mask.encoder.layer.10.attention.self.value.weight\", \"esm_mask.encoder.layer.10.attention.self.value.bias\", \"esm_mask.encoder.layer.10.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.10.attention.output.dense.weight\", \"esm_mask.encoder.layer.10.attention.output.dense.bias\", \"esm_mask.encoder.layer.10.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.10.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.10.intermediate.dense.weight\", \"esm_mask.encoder.layer.10.intermediate.dense.bias\", \"esm_mask.encoder.layer.10.output.dense.weight\", \"esm_mask.encoder.layer.10.output.dense.bias\", \"esm_mask.encoder.layer.10.LayerNorm.weight\", \"esm_mask.encoder.layer.10.LayerNorm.bias\", \"esm_mask.encoder.layer.11.attention.self.query.weight\", \"esm_mask.encoder.layer.11.attention.self.query.bias\", \"esm_mask.encoder.layer.11.attention.self.key.weight\", \"esm_mask.encoder.layer.11.attention.self.key.bias\", \"esm_mask.encoder.layer.11.attention.self.value.weight\", \"esm_mask.encoder.layer.11.attention.self.value.bias\", \"esm_mask.encoder.layer.11.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.encoder.layer.11.attention.output.dense.weight\", \"esm_mask.encoder.layer.11.attention.output.dense.bias\", \"esm_mask.encoder.layer.11.attention.LayerNorm.weight\", \"esm_mask.encoder.layer.11.attention.LayerNorm.bias\", \"esm_mask.encoder.layer.11.intermediate.dense.weight\", \"esm_mask.encoder.layer.11.intermediate.dense.bias\", \"esm_mask.encoder.layer.11.output.dense.weight\", \"esm_mask.encoder.layer.11.output.dense.bias\", \"esm_mask.encoder.layer.11.LayerNorm.weight\", \"esm_mask.encoder.layer.11.LayerNorm.bias\", \"esm_mask.encoder.emb_layer_norm_after.weight\", \"esm_mask.encoder.emb_layer_norm_after.bias\", \"esm_mask.pooler.dense.weight\", \"esm_mask.pooler.dense.bias\". \n\tUnexpected key(s) in state_dict: \"esm_mask.esm.embeddings.word_embeddings.weight\", \"esm_mask.esm.embeddings.position_embeddings.weight\", \"esm_mask.esm.encoder.layer.0.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.0.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.0.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.0.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.0.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.0.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.0.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.0.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.0.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.0.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.0.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.0.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.0.output.dense.weight\", \"esm_mask.esm.encoder.layer.0.output.dense.bias\", \"esm_mask.esm.encoder.layer.0.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.0.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.1.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.1.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.1.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.1.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.1.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.1.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.1.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.1.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.1.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.1.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.1.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.1.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.1.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.1.output.dense.weight\", \"esm_mask.esm.encoder.layer.1.output.dense.bias\", \"esm_mask.esm.encoder.layer.1.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.1.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.2.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.2.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.2.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.2.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.2.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.2.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.2.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.2.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.2.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.2.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.2.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.2.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.2.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.2.output.dense.weight\", \"esm_mask.esm.encoder.layer.2.output.dense.bias\", \"esm_mask.esm.encoder.layer.2.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.2.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.3.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.3.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.3.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.3.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.3.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.3.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.3.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.3.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.3.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.3.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.3.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.3.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.3.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.3.output.dense.weight\", \"esm_mask.esm.encoder.layer.3.output.dense.bias\", \"esm_mask.esm.encoder.layer.3.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.3.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.4.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.4.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.4.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.4.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.4.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.4.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.4.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.4.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.4.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.4.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.4.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.4.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.4.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.4.output.dense.weight\", \"esm_mask.esm.encoder.layer.4.output.dense.bias\", \"esm_mask.esm.encoder.layer.4.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.4.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.5.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.5.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.5.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.5.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.5.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.5.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.5.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.5.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.5.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.5.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.5.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.5.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.5.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.5.output.dense.weight\", \"esm_mask.esm.encoder.layer.5.output.dense.bias\", \"esm_mask.esm.encoder.layer.5.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.5.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.6.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.6.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.6.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.6.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.6.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.6.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.6.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.6.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.6.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.6.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.6.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.6.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.6.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.6.output.dense.weight\", \"esm_mask.esm.encoder.layer.6.output.dense.bias\", \"esm_mask.esm.encoder.layer.6.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.6.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.7.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.7.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.7.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.7.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.7.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.7.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.7.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.7.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.7.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.7.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.7.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.7.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.7.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.7.output.dense.weight\", \"esm_mask.esm.encoder.layer.7.output.dense.bias\", \"esm_mask.esm.encoder.layer.7.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.7.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.8.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.8.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.8.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.8.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.8.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.8.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.8.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.8.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.8.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.8.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.8.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.8.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.8.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.8.output.dense.weight\", \"esm_mask.esm.encoder.layer.8.output.dense.bias\", \"esm_mask.esm.encoder.layer.8.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.8.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.9.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.9.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.9.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.9.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.9.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.9.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.9.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.9.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.9.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.9.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.9.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.9.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.9.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.9.output.dense.weight\", \"esm_mask.esm.encoder.layer.9.output.dense.bias\", \"esm_mask.esm.encoder.layer.9.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.9.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.10.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.10.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.10.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.10.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.10.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.10.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.10.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.10.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.10.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.10.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.10.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.10.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.10.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.10.output.dense.weight\", \"esm_mask.esm.encoder.layer.10.output.dense.bias\", \"esm_mask.esm.encoder.layer.10.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.10.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.11.attention.self.query.weight\", \"esm_mask.esm.encoder.layer.11.attention.self.query.bias\", \"esm_mask.esm.encoder.layer.11.attention.self.key.weight\", \"esm_mask.esm.encoder.layer.11.attention.self.key.bias\", \"esm_mask.esm.encoder.layer.11.attention.self.value.weight\", \"esm_mask.esm.encoder.layer.11.attention.self.value.bias\", \"esm_mask.esm.encoder.layer.11.attention.self.rotary_embeddings.inv_freq\", \"esm_mask.esm.encoder.layer.11.attention.output.dense.weight\", \"esm_mask.esm.encoder.layer.11.attention.output.dense.bias\", \"esm_mask.esm.encoder.layer.11.attention.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.11.attention.LayerNorm.bias\", \"esm_mask.esm.encoder.layer.11.intermediate.dense.weight\", \"esm_mask.esm.encoder.layer.11.intermediate.dense.bias\", \"esm_mask.esm.encoder.layer.11.output.dense.weight\", \"esm_mask.esm.encoder.layer.11.output.dense.bias\", \"esm_mask.esm.encoder.layer.11.LayerNorm.weight\", \"esm_mask.esm.encoder.layer.11.LayerNorm.bias\", \"esm_mask.esm.encoder.emb_layer_norm_after.weight\", \"esm_mask.esm.encoder.emb_layer_norm_after.bias\", \"esm_mask.esm.contact_head.regression.weight\", \"esm_mask.esm.contact_head.regression.bias\", \"esm_mask.lm_head.bias\", \"esm_mask.lm_head.dense.weight\", \"esm_mask.lm_head.dense.bias\", \"esm_mask.lm_head.layer_norm.weight\", \"esm_mask.lm_head.layer_norm.bias\", \"esm_mask.lm_head.decoder.weight\". "
     ]
    }
   ],
   "source": [
    "load_model = torch.load(f\"{folder_huggingface_download}/pytorch_model.bin\")\n",
    "PLMinter.load_state_dict(load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[protein1, protein2]\n",
    "tokenized = tokenizer(*texts, padding=True, truncation='longest_first', return_tensors=\"pt\", max_length=1603)       \n",
    "tokenized = tokenized.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4626026153564453\n"
     ]
    }
   ],
   "source": [
    "PLMinter.eval()\n",
    "PLMinter.to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    probability = PLMinter.forward_test(tokenized)\n",
    "    print(probability.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PLMinteract(\n",
       "  (esm_mask): EsmModel(\n",
       "    (embeddings): EsmEmbeddings(\n",
       "      (word_embeddings): Embedding(33, 480, padding_idx=1)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (position_embeddings): Embedding(1026, 480, padding_idx=1)\n",
       "    )\n",
       "    (encoder): EsmEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (emb_layer_norm_after): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (pooler): EsmPooler(\n",
       "      (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=480, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PLMinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentence_pair",
   "language": "python",
   "name": "sentence_pair"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
